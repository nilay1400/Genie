{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f3af5c-4631-4ce5-aef4-873fbdfba668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb011360-1036-4ac9-816c-0563de679948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# this is one of Hyper parameter, but let's select given below value\n",
    "batch_size = 512\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "# this will help us to create Grid of images\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size = 5),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size = 2),\n",
    "            nn.Conv2d(6, 16, kernel_size = 5),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size = 2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(84, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logit = self.classifier(x)\n",
    "        return logit\n",
    "\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    _, preds = torch.max(output, dim = 1)\n",
    "\n",
    "    return torch.sum(preds == labels).item() / len(preds)\n",
    "\n",
    "\n",
    "device = get_default_device()\n",
    "device\n",
    "\n",
    "def eevaluate(model, loss_fn, val_dl, metric = None, device='cuda'):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        results = [loss_batch(model, loss_fn, x, y, metric = metric) for x, y in val_dl]\n",
    "\n",
    "        losses, nums, metrics = zip(*results)\n",
    "\n",
    "        total = np.sum(nums)\n",
    "\n",
    "        avg_loss = np.sum(np.multiply(losses, nums)) / total\n",
    "\n",
    "        avg_metric = None\n",
    "\n",
    "        if metric is not None:\n",
    "            avg_metric = np.sum(np.multiply(metrics, nums)) / total\n",
    "\n",
    "    return avg_loss, total, avg_metric\n",
    "\n",
    "def loss_batch(model, loss_func, x, y, opt = None, metric = None):\n",
    "\n",
    "    pred = model(x)\n",
    "\n",
    "    loss = loss_func(pred, y)\n",
    "\n",
    "    if opt is not None:\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    metric_result = None\n",
    "\n",
    "    if metric is not None:\n",
    "\n",
    "        metric_result = metric(pred, y)\n",
    "\n",
    "    return loss.item(), len(x), metric_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25d69939-2a42-4711-8b48-570910225390",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('lenet.pth', map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7027ca4-0af5-44b7-b23e-abcdb3c418df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "# transform is used to convert data into Tensor form with transformations\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aa8ad4d-f924-4cba-acdd-d5c30aae74cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Using downloaded and verified file: ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Using downloaded and verified file: ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Using downloaded and verified file: ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    # To resize image\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "    # To normalize image\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "root = './data',\n",
    "train = True,\n",
    "download = True,\n",
    "transform = trans\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "root = './data',\n",
    "train = False,\n",
    "download = True,\n",
    "transform = trans\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5cecedf-5add-4bf1-8e34-fb1ba0ebbcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Losses: 0.03845255719994893, Accuracy: 98.9\n"
     ]
    }
   ],
   "source": [
    "test_loader = DeviceDataLoader(DataLoader(test_set, batch_size=256), device)\n",
    "result = eevaluate(model, F.cross_entropy, test_loader, metric = accuracy)\n",
    "result\n",
    "Accuracy = result[2] * 100\n",
    "Accuracy\n",
    "loss = result[0]\n",
    "print(\"Total Losses: {}, Accuracy: {}\".format(loss, Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b84e830e-538f-4127-92a1-52387e852962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting deap\n",
      "  Downloading deap-1.4.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.0/135.0 kB\u001b[0m \u001b[31m697.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from deap) (1.19.5)\n",
      "Installing collected packages: deap\n",
      "Successfully installed deap-1.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install deap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "179820fd-447e-4d0d-9b03-763f16280761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bc1a975-b26f-4359-afd4-3e53718780d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')\n",
    "# Define the evaluation function\n",
    "def evaluate(individual):\n",
    "    # print(\"Evaluating individual:\", individual)\n",
    "    model_copy = torch.load('lenet.pth')\n",
    "    model_copy = model_copy.to('cuda')\n",
    "\n",
    "###################################################################\n",
    "\n",
    "    state_dict = model_copy.state_dict()\n",
    "    for layer_name, weight_idx in individual:\n",
    "        weight = state_dict[layer_name].view(-1).to('cuda')\n",
    "        # print(weight)\n",
    "        # weight[weight_idx] += 0.01  # Perturb the weight slightly\n",
    "        \n",
    "        weight[weight_idx] = weight[weight_idx] + 0.1\n",
    "\n",
    "\n",
    "    # Load the perturbed weights back into the model\n",
    "    model_copy.load_state_dict(state_dict)\n",
    "\n",
    "    # Evaluate the perturbed model on a validation set\n",
    "    model_copy.eval()\n",
    "    result = eevaluate(model_copy, F.cross_entropy, test_loader, metric = accuracy)\n",
    "    Accuracy = result[2] * 100\n",
    "    loss = result[0]\n",
    "\n",
    "\n",
    "    # Return the loss as fitness (higher loss indicates more critical weight)\n",
    "    return loss,\n",
    "\n",
    "# Create the fitness and individual classes\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  # Maximize the function\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "def custom_mutate(individual, indpb):\n",
    "    # print(\"Before mutation:\", individual)\n",
    "    for i in range(len(individual)):\n",
    "        if random.random() < indpb:\n",
    "            layer, index = individual[i]\n",
    "            new_index = random.randint(0, num_weights_per_layer[layer] - 1)\n",
    "            individual[i] = (layer, new_index)\n",
    "    # print(\"After mutation:\", individual)\n",
    "    return individual,\n",
    "\n",
    "def custom_crossover(ind1, ind2):\n",
    "    # print(\"Before crossover:\", ind1, ind2)\n",
    "    tools.cxTwoPoint(ind1, ind2)\n",
    "    # print(\"After crossover:\", ind1, ind2)\n",
    "    return ind1, ind2\n",
    "\n",
    "\n",
    "# Attribute generator: (layer, index) pair\n",
    "layer_names = [name for name in model.state_dict().keys() if 'weight' in name]\n",
    "num_weights_per_layer = {name: model.state_dict()[name].numel() for name in layer_names}\n",
    "def random_weight():\n",
    "    layer = random.choice(layer_names)\n",
    "    index = random.randint(0, num_weights_per_layer[layer] - 1)\n",
    "    # print(layer,index)\n",
    "    return (layer, index)\n",
    "\n",
    "# Structure initializers\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, random_weight, n=10)  # Each individual perturbs 5 weights\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Register the genetic operators\n",
    "toolbox.register(\"mutate\", custom_mutate, indpb=0.2)\n",
    "toolbox.register(\"mate\", custom_crossover)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"evaluate\", evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a469af1-bd2c-40b3-8c60-be27fe0debdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg      \tmin      \tmax      \n",
      "0  \t100   \t0.0385178\t0.0377656\t0.0394711\n",
      "Generation 1\n",
      "1  \t46    \t0.038813 \t0.0381003\t0.0397409\n",
      "Generation 2\n",
      "2  \t55    \t0.0390459\t0.0383562\t0.0400736\n",
      "Generation 3\n",
      "3  \t68    \t0.039303 \t0.0383513\t0.0404907\n",
      "Generation 4\n",
      "4  \t62    \t0.0395477\t0.0385314\t0.0410091\n",
      "Generation 5\n",
      "5  \t67    \t0.0397567\t0.0386112\t0.0406757\n",
      "Generation 6\n",
      "6  \t59    \t0.0399772\t0.038381 \t0.0409916\n",
      "Generation 7\n",
      "7  \t68    \t0.0403472\t0.038506 \t0.0418911\n",
      "Generation 8\n",
      "8  \t63    \t0.0406816\t0.0391649\t0.0421289\n",
      "Generation 9\n",
      "9  \t65    \t0.0410157\t0.0388086\t0.0426386\n",
      "Generation 10\n",
      "10 \t65    \t0.0413509\t0.0386242\t0.0427194\n",
      "Generation 11\n",
      "11 \t62    \t0.0417474\t0.0401175\t0.0427194\n",
      "Generation 12\n",
      "12 \t58    \t0.0418611\t0.039271 \t0.0429701\n",
      "Generation 13\n",
      "13 \t70    \t0.0421259\t0.0400626\t0.0430362\n",
      "Generation 14\n",
      "14 \t62    \t0.0423104\t0.0395592\t0.0433796\n",
      "Generation 15\n",
      "15 \t66    \t0.0425175\t0.0399915\t0.0435805\n",
      "Generation 16\n",
      "16 \t61    \t0.0428122\t0.0409749\t0.0435805\n",
      "Generation 17\n",
      "17 \t62    \t0.0429603\t0.0399127\t0.0436754\n",
      "Generation 18\n",
      "18 \t65    \t0.0430165\t0.0399318\t0.0436395\n",
      "Generation 19\n",
      "22 \t48    \t0.0432969\t0.0402704\t0.0435967\n",
      "Generation 23\n",
      "23 \t59    \t0.0434018\t0.0412172\t0.0435967\n",
      "Generation 24\n",
      "24 \t67    \t0.0434202\t0.0402219\t0.0435967\n",
      "Generation 25\n",
      "25 \t67    \t0.0432327\t0.0401758\t0.0436395\n",
      "Generation 26\n",
      "26 \t57    \t0.0433974\t0.0407364\t0.0436395\n",
      "Generation 27\n",
      "27 \t74    \t0.043332 \t0.0405902\t0.0436395\n",
      "Generation 28\n",
      "28 \t55    \t0.0433659\t0.0400889\t0.0436687\n",
      "Generation 29\n",
      "29 \t58    \t0.043263 \t0.0406807\t0.0437928\n",
      "Generation 30\n",
      "30 \t60    \t0.0433223\t0.0406871\t0.043815 \n",
      "Generation 31\n",
      "31 \t58    \t0.0434139\t0.0411354\t0.043815 \n",
      "Generation 32\n",
      "32 \t65    \t0.043416 \t0.039348 \t0.043815 \n",
      "Generation 33\n",
      "33 \t56    \t0.0435179\t0.0406388\t0.043815 \n",
      "Generation 34\n"
     ]
    }
   ],
   "source": [
    "from deap import tools, base, creator, algorithms\n",
    "\n",
    "def eaSimpleWithDebugging(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "                          halloffame=None, verbose=__debug__):\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "    if halloffame is not None:\n",
    "        halloffame.update(population)\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "        print(f\"Generation {gen}\")\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population))\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "        # Apply crossover and mutation on the offspring\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() < cxpb:\n",
    "                # print(f\"Before mate: {child1}, {child2}\")\n",
    "                toolbox.mate(child1, child2)\n",
    "                # print(f\"After mate: {child1}, {child2}\")\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "\n",
    "        for mutant in offspring:\n",
    "            if random.random() < mutpb:\n",
    "                # print(f\"Before mutate: {mutant}\")\n",
    "                toolbox.mutate(mutant)\n",
    "                # print(f\"After mutate: {mutant}\")\n",
    "                del mutant.fitness.values\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        if halloffame is not None:\n",
    "            halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook\n",
    "\n",
    "\n",
    "def main():\n",
    "    random.seed(42)\n",
    "\n",
    "    # Create an initial population of 100 individuals\n",
    "    population = toolbox.population(n=100)\n",
    "\n",
    "    # Define statistics to keep track of the progress\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values[0])  # Extract the first element of the fitness tuple\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"min\", min)\n",
    "    stats.register(\"max\", max)\n",
    "\n",
    "    # Hall of Fame to keep the best individual\n",
    "    hof = tools.HallOfFame(1)\n",
    "\n",
    "    # Run the genetic algorithm\n",
    "    population, logbook = eaSimpleWithDebugging(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=40,\n",
    "                                                stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # Print the best individual\n",
    "    print(\"Best individual is: \", hof[0])\n",
    "    print(\"Fitness: \", hof[0].fitness.values[0])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd3163-59e2-4649-a031-e8cc3b4f1a42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
